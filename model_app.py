"""model_app

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nMj9rMh13rlXM_rrez-_HvF63AHuW8o-
"""

import json
from pathlib import Path
from collections import Counter
import random
import textwrap

import streamlit as st
import evaluate  # make sure to install: pip install evaluate rouge_score


# ===============================
# 1. DATA LOADING
# ===============================

@st.cache_data
def load_qa_data(path: str = "Q&A_db_practice.json"):
    data_path = Path(path)
    if not data_path.exists():
        st.error(f"Could not find {path}. Please make sure the file is in the same folder as model_app.py.")
        return []
    with open(data_path, "r", encoding="utf-8") as f:
        qa_items = json.load(f)
    return qa_items


qa_items = load_qa_data()

if not qa_items:
    st.stop()


# ===============================
# 2. AUTOMATIC EVALUATOR
# ===============================

# Initialize ROUGE metric
rouge = evaluate.load("rouge")


def compute_rouge_l(reference: str, prediction: str) -> float:
    """
    Compute ROUGE-L F1 score between reference and prediction.
    Returns a value between 0 and 1.
    """
    results = rouge.compute(
        predictions=[prediction],
        references=[reference],
        use_stemmer=True
    )
    return results["rougeL"]


def extract_keywords(text: str, min_len: int = 4, top_k: int = 20):
    """
    Extract simple keyword candidates: lowercase tokens with at least min_len chars,
    ranked by frequency (very naive but sufficient for this assignment).
    """
    tokens = [t.strip(".,;:!?()[]\"'").lower() for t in text.split()]
    tokens = [t for t in tokens if len(t) >= min_len]
    freq = Counter(tokens)
    most_common = [w for w, _ in freq.most_common(top_k)]
    return most_common


def keyword_coverage(reference: str, prediction: str, top_k: int = 20):
    """
    Compute coverage of important reference keywords in the student's prediction.
    Returns:
        coverage_ratio (0-1),
        present_keywords,
        missing_keywords
    """
    ref_keywords = extract_keywords(reference, top_k=top_k)
    pred_tokens = set(
        t.strip(".,;:!?()[]\"'").lower() for t in prediction.split()
    )

    present = [w for w in ref_keywords if w in pred_tokens]
    missing = [w for w in ref_keywords if w not in pred_tokens]

    coverage = len(present) / max(1, len(ref_keywords))
    return coverage, present, missing


def evaluate_answer(reference: str, student_answer: str) -> dict:
    """
    Main automatic evaluator:
    - Computes ROUGE-L
    - Computes keyword coverage
    - Produces numeric score (0-100)
    - Produces textual explanation
    """
    # 1. ROUGE-L (0-1)
    rouge_l = compute_rouge_l(reference, student_answer)

    # 2. Keyword coverage (0-1)
    coverage, present_kw, missing_kw = keyword_coverage(reference, student_answer)

    # 3. Combine the two into a score (simple linear combination)
    alpha = 0.5  # weight for ROUGE-L
    beta = 0.5   # weight for keyword coverage

    combined = alpha * rouge_l + beta * coverage
    score_0_100 = round(combined * 100, 1)

    # 4. Build explanation
    explanation_parts = []

    explanation_parts.append(
        f"ROUGE-L similarity: {rouge_l:.3f} (0‚Äì1 scale). "
        f"This reflects overlap in phrasing and sentence structure."
    )
    explanation_parts.append(
        f"Keyword coverage: {coverage:.3f} (0‚Äì1 scale). "
        f"This reflects how many core terms from the reference you mentioned."
    )

    if missing_kw:
        missing_str = ", ".join(missing_kw[:8])
        explanation_parts.append(
            f"Some important concepts not explicitly mentioned: {missing_str}."
        )
    if present_kw:
        present_str = ", ".join(present_kw[:8])
        explanation_parts.append(
            f"Key concepts you did mention: {present_str}."
        )

    explanation = " ".join(explanation_parts)

    return {
        "score": score_0_100,
        "rouge_l": rouge_l,
        "coverage": coverage,
        "present_keywords": present_kw,
        "missing_keywords": missing_kw,
        "explanation": explanation,
    }


# ===============================
# 3. OPTIONAL LLM-BASED EVALUATOR (STUB)
# ===============================

LLM_SYSTEM_PROMPT = """
You are an expert Machine Learning professor.
The student is answering conceptual questions.
Your task is to:
1) Compare the student's answer with the reference answer.
2) Briefly explain what is correct, what is missing, and what is wrong.
3) Provide a numeric score from 0 to 100.

You MUST respond in valid JSON with the following keys:
- "score": number from 0 to 100
- "analysis": short textual explanation
"""


def build_llm_prompt(question: str, reference_answer: str, student_answer: str) -> str:
    return f"""
[QUESTION]
{question}

[REFERENCE ANSWER]
{reference_answer}

[STUDENT ANSWER]
{student_answer}

Now compare the student's answer to the reference and respond as requested.
"""


def call_llm_evaluator(prompt: str) -> dict:
    """
    Stub for LLM-based evaluation.
    - For the assignment, you can either:
      (a) leave this stub with a dummy output, or
      (b) replace it with a call to OpenAI / HuggingFace / Ollama.
    """
    # DUMMY implementation to keep the app running without an API:
    # You can replace everything below with a real LLM call if you want.
    dummy_response = {
        "score": 75.0,
        "analysis": (
            "This is a placeholder LLM evaluation. "
            "Replace 'call_llm_evaluator' with a real model call to use an actual LLM judge."
        ),
    }
    return dummy_response


def evaluate_answer_llm(question: str, reference: str, student_answer: str) -> dict:
    """
    Uses an LLM as a judge for the student's answer.
    Returns a dict with:
    - score (0‚Äì100)
    - analysis (text explanation)
    """
    prompt = build_llm_prompt(question, reference, student_answer)
    llm_response = call_llm_evaluator(prompt)
    return llm_response


# ===============================
# 4. QUESTION SAMPLING & STATE
# ===============================

def sample_question(qa_list):
    return random.choice(qa_list)


if "current_qa" not in st.session_state:
    st.session_state.current_qa = None

if "history" not in st.session_state:
    st.session_state.history = []  # list of dicts


# ===============================
# 5. STREAMLIT UI
# ===============================

st.set_page_config(
    page_title="ML Q&A Evaluator",
    page_icon="ü§ñ",
    layout="wide",
)

st.title("ü§ñ ML Concept Q&A Evaluator")
st.write(
    "This prototype asks you questions about Machine Learning concepts, "
    "collects your answer, and evaluates it automatically (ROUGE + keyword coverage). "
    "Optionally, it also shows a placeholder LLM-based evaluation."
)

with st.sidebar:
    st.header("Controls")
    if st.button("üîÑ New random question"):
        st.session_state.current_qa = sample_question(qa_items)
        # Clear previous answer/result for clarity
        if "last_answer" in st.session_state:
            del st.session_state["last_answer"]
        if "last_eval_auto" in st.session_state:
            del st.session_state["last_eval_auto"]
        if "last_eval_llm" in st.session_state:
            del st.session_state["last_eval_llm"]

    if st.button("üßº Reset session"):
        st.session_state.current_qa = None
        st.session_state.history = []
        if "last_answer" in st.session_state:
            del st.session_state["last_answer"]
        if "last_eval_auto" in st.session_state:
            del st.session_state["last_eval_auto"]
        if "last_eval_llm" in st.session_state:
            del st.session_state["last_eval_llm"]

    st.markdown("---")
    st.subheader("Session stats")
    st.write(f"Questions answered: **{len(st.session_state.history)}**")
    if st.session_state.history:
        avg_score = sum(h["evaluation"]["automatic"]["score"] for h in st.session_state.history) / len(st.session_state.history)
        st.write(f"Average automatic score: **{avg_score:.1f} / 100**")

    st.markdown("---")
    st.caption("Prototype for Assignment 11.00 ‚Äì LLM Evaluator (Streamlit UI).")


# If there is no current question, sample one
if st.session_state.current_qa is None:
    st.session_state.current_qa = sample_question(qa_items)

current_qa = st.session_state.current_qa
question = current_qa["question"]
reference_answer = current_qa["answer"]

st.subheader("Current Question")
st.write(question)

st.markdown("### Your Answer")
default_text = st.session_state.get("last_answer", "")
student_answer = st.text_area(
    "Type your answer here:",
    value=default_text,
    height=160,
    placeholder="Write your explanation in your own words...",
)

col1, col2 = st.columns([1, 2])

with col1:
    submit_clicked = st.button("‚úÖ Submit answer")

with col2:
    show_reference = st.checkbox("Show reference answer after evaluation")


if submit_clicked and student_answer.strip():
    # Store last answer
    st.session_state.last_answer = student_answer

    # 1) Automatic evaluation
    eval_auto = evaluate_answer(reference_answer, student_answer)

    # 2) LLM-based evaluation (stub, but structured)
    eval_llm = evaluate_answer_llm(question, reference_answer, student_answer)

    # Store last evals in session
    st.session_state.last_eval_auto = eval_auto
    st.session_state.last_eval_llm = eval_llm

    # Add to history
    st.session_state.history.append(
        {
            "question": question,
            "reference_answer": reference_answer,
            "student_answer": student_answer,
            "evaluation": {
                "automatic": eval_auto,
                "llm": eval_llm,
            },
        }
    )

# Display results if available
if "last_eval_auto" in st.session_state:
    eval_auto = st.session_state.last_eval_auto
    eval_llm = st.session_state.last_eval_llm

    st.markdown("## Evaluation Results")

    # Automatic evaluation
    st.markdown("### üîç Automatic Evaluation (ROUGE + Keywords)")
    st.write(f"**Score (0‚Äì100):** `{eval_auto['score']}`")
    st.write(
        textwrap.fill(eval_auto["explanation"], width=100)
    )

    # LLM-based evaluation
    st.markdown("### üß† LLM-based Evaluation (Stub)")
    st.write(f"**LLM Score (0‚Äì100):** `{eval_llm['score']}`")
    st.write(
        textwrap.fill(eval_llm["analysis"], width=100)
    )

    # Reference answer (optional)
    if show_reference:
        st.markdown("### üìò Reference Answer")
        st.write(textwrap.fill(reference_answer, width=100))
